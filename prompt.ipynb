{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:57:29.005077Z",
     "start_time": "2025-07-31T13:57:27.268275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "llm.invoke(\"what is Capital of Korea?\")"
   ],
   "id": "fd309da0cb410b37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There are actually two countries called \"Korea\":\\n\\n1. **South Korea** (officially known as the Republic of Korea): The capital of South Korea is **Seoul**.\\n2. **North Korea** (officially known as the Democratic People\\'s Republic of Korea): The capital of North Korea is **Pyongyang**.\\n\\nSo, which one were you thinking of?', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-07-31T13:57:29.0024312Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1188366200, 'load_duration': 24451400, 'prompt_eval_count': 16, 'prompt_eval_duration': 2073700, 'eval_count': 77, 'eval_duration': 1161841100, 'model_name': 'llama3.1'}, id='run--404c8de8-23f7-41f6-a27e-4c614853bfd9-0', usage_metadata={'input_tokens': 16, 'output_tokens': 77, 'total_tokens': 93})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "invoke = prompt",
   "id": "6945cfbf2813071a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:57:33.268050Z",
     "start_time": "2025-07-31T13:57:33.236920Z"
    }
   },
   "cell_type": "code",
   "source": "llm.invoke(0)",
   "id": "76f40aefb4f63acb",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'int'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\llmBasic\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:396\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    383\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    384\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    385\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    390\u001B[39m     **kwargs: Any,\n\u001B[32m    391\u001B[39m ) -> BaseMessage:\n\u001B[32m    392\u001B[39m     config = ensure_config(config)\n\u001B[32m    393\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    394\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    395\u001B[39m         \u001B[38;5;28mself\u001B[39m.generate_prompt(\n\u001B[32m--> \u001B[39m\u001B[32m396\u001B[39m             [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m],\n\u001B[32m    397\u001B[39m             stop=stop,\n\u001B[32m    398\u001B[39m             callbacks=config.get(\u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    399\u001B[39m             tags=config.get(\u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    400\u001B[39m             metadata=config.get(\u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    401\u001B[39m             run_name=config.get(\u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    402\u001B[39m             run_id=config.pop(\u001B[33m\"\u001B[39m\u001B[33mrun_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m    403\u001B[39m             **kwargs,\n\u001B[32m    404\u001B[39m         ).generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    405\u001B[39m     ).message\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\llmBasic\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:381\u001B[39m, in \u001B[36mBaseChatModel._convert_input\u001B[39m\u001B[34m(self, model_input)\u001B[39m\n\u001B[32m    376\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatPromptValue(messages=convert_to_messages(model_input))\n\u001B[32m    377\u001B[39m msg = (\n\u001B[32m    378\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInvalid input type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(model_input)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    379\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mMust be a PromptValue, str, or list of BaseMessages.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    380\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m381\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n",
      "\u001B[31mValueError\u001B[39m: Invalid input type <class 'int'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "prompt의 타입 ValueError: Invalid input type <class 'int'>. Must be a PromptValue, str, or list of BaseMessages.\n",
    "\n",
    "Prompt Template\n",
    "- PromptValue를 만들 수 있음"
   ],
   "id": "c61f67ad24111db6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:57:35.593395Z",
     "start_time": "2025-07-31T13:57:35.581674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"what is the capital of {country}\",\n",
    "    input_variable=[\"country\"],\n",
    ")"
   ],
   "id": "11a1e1558eb251",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "{country} => placeholder => template literal의 ${}와 같은 역할 => 값 대입을 어떻게 하냐? => prompt_template에 invoke를 하여 사용",
   "id": "d534bb09069a609f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:57:37.640954Z",
     "start_time": "2025-07-31T13:57:37.636531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = prompt_template.invoke({\"country\": \"France\"}) \n",
    "\n",
    "print(prompt)"
   ],
   "id": "6019593545018f1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='what is the capital of France'\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "prompt: PromptValue => prompt_template의 type",
   "id": "b56151f100ff75f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:57:40.430073Z",
     "start_time": "2025-07-31T13:57:39.180426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm.invoke(prompt_template.invoke({\n",
    "    \"country\": \"Korea\",\n",
    "}))"
   ],
   "id": "5a5b0bb871c7039d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"There are actually two Koreas: North Korea (officially known as the Democratic People's Republic of Korea) and South Korea (officially known as the Republic of Korea).\\n\\n* The capital of **South Korea** is Seoul.\\n* The capital of **North Korea** is Pyongyang.\\n\\nSo, if you're referring to one or the other, I'd be happy to help clarify!\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-07-31T13:57:40.4277644Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1241668600, 'load_duration': 26870500, 'prompt_eval_count': 16, 'prompt_eval_duration': 155793200, 'eval_count': 78, 'eval_duration': 1059004900, 'model_name': 'llama3.1'}, id='run--1641e2bf-2751-452f-8e34-50afa6d45327-0', usage_metadata={'input_tokens': 16, 'output_tokens': 78, 'total_tokens': 94})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prompt_template을 invoke한 결과를 llm에 invoke한 결과가 AI의 답변이다.",
   "id": "9f1f96306d9420e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:57:41.130903Z",
     "start_time": "2025-07-31T13:57:41.026810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "message_list = [\n",
    "    SystemMessage(content = \"You are a helpful assistant!\"),\n",
    "    HumanMessage(content=\"What is the capital of Germany\"),\n",
    "    AIMessage(content=\"The capital of Germany\"),\n",
    "    HumanMessage(content=\"The capital of {country}\"),\n",
    "]\n",
    "\n",
    "llm.invoke(message_list)"
   ],
   "id": "b2e5463a7d8f4bd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Germany: Berlin.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-07-31T13:57:41.1282394Z', 'done': True, 'done_reason': 'stop', 'total_duration': 96863600, 'load_duration': 24415500, 'prompt_eval_count': 47, 'prompt_eval_duration': 8656400, 'eval_count': 5, 'eval_duration': 63278500, 'model_name': 'llama3.1'}, id='run--05082202-b697-42e6-b29f-8f0e5421072a-0', usage_metadata={'input_tokens': 47, 'output_tokens': 5, 'total_tokens': 52})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "HumanMessage는 BaseMessage를 상속 받음\n",
    "\n",
    "BaseMessage를 상속 받는 4가지\n",
    "\n",
    "1. system => LLM APP의 목적 => 페르소나 => LLM이 APP에서 할 목적을 정의함\n",
    "2. Human => user => 질문의 예제\n",
    "3. AI => LLM => 답변의 예제\n",
    "4. Tool => agent \n",
    "\n",
    "few_shots\n",
    "one_shot\n",
    "- shot => 예제 => LLM을 속여서 USER가 원하는 답변을 도출한다."
   ],
   "id": "89933cc6db61976f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:57:43.789867Z",
     "start_time": "2025-07-31T13:57:43.770864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# langChain의 ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant!\"),\n",
    "    (\"human\", \"The capital of {country}\"),\n",
    "])\n",
    "\n",
    "chat_prompt = chat_prompt_template.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(chat_prompt)"
   ],
   "id": "ebce137f77584f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='The capital of France', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "langchain의 chatprompt template을 사용할 경우 tuple을 이용해야 한다.\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant!\"),\n",
    "    (\"human\", \"The capital of {country}\"),\n",
    "])\n"
   ],
   "id": "d14d017ddda5e49e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:57:45.947536Z",
     "start_time": "2025-07-31T13:57:45.943536Z"
    }
   },
   "cell_type": "code",
   "source": "chat_prompt.messages",
   "id": "292ae292a7fefb26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='The capital of France', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T13:57:47.114164Z",
     "start_time": "2025-07-31T13:57:46.724439Z"
    }
   },
   "cell_type": "code",
   "source": "llm.invoke(chat_prompt)",
   "id": "ed82c53177648ff9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-07-31T13:57:47.1117437Z', 'done': True, 'done_reason': 'stop', 'total_duration': 382430300, 'load_duration': 29521400, 'prompt_eval_count': 25, 'prompt_eval_duration': 63473800, 'eval_count': 8, 'eval_duration': 289435100, 'model_name': 'llama3.1'}, id='run--9a8af277-0ae5-4568-9968-3365d0124b16-0', usage_metadata={'input_tokens': 25, 'output_tokens': 8, 'total_tokens': 33})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c611b575c5e0b0c3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
