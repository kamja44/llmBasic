{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:31:56.403229Z",
     "start_time": "2025-07-31T07:31:45.781096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "llm.invoke(\"what is Capital of Korea?\")"
   ],
   "id": "fd309da0cb410b37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You're likely referring to one of two countries: North Korea or South Korea. Both countries have their own capitals.\\n\\n1. **South Korea**: The capital of South Korea (officially the Republic of Korea) is Seoul.\\n2. **North Korea**: The capital of North Korea (officially the Democratic People's Republic of Korea) is Pyongyang.\\n\\nSo, which one were you asking about?\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-07-31T07:31:56.394612Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10006834800, 'load_duration': 4740713800, 'prompt_eval_count': 16, 'prompt_eval_duration': 323763400, 'eval_count': 80, 'eval_duration': 4941801100, 'model_name': 'llama3.1'}, id='run--d2d3271a-7fc5-4a5e-8f8c-76c32e68d13f-0', usage_metadata={'input_tokens': 16, 'output_tokens': 80, 'total_tokens': 96})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "invoke = prompt",
   "id": "6945cfbf2813071a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:32:34.224585Z",
     "start_time": "2025-07-31T07:32:34.061871Z"
    }
   },
   "cell_type": "code",
   "source": "llm.invoke(0)",
   "id": "76f40aefb4f63acb",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'int'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m llm.invoke(\u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\llmBasic\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:396\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    383\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    384\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    385\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    390\u001B[39m     **kwargs: Any,\n\u001B[32m    391\u001B[39m ) -> BaseMessage:\n\u001B[32m    392\u001B[39m     config = ensure_config(config)\n\u001B[32m    393\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    394\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mChatGeneration\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    395\u001B[39m         \u001B[38;5;28mself\u001B[39m.generate_prompt(\n\u001B[32m--> \u001B[39m\u001B[32m396\u001B[39m             [\u001B[38;5;28mself\u001B[39m._convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[32m    397\u001B[39m             stop=stop,\n\u001B[32m    398\u001B[39m             callbacks=config.get(\u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    399\u001B[39m             tags=config.get(\u001B[33m\"\u001B[39m\u001B[33mtags\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    400\u001B[39m             metadata=config.get(\u001B[33m\"\u001B[39m\u001B[33mmetadata\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    401\u001B[39m             run_name=config.get(\u001B[33m\"\u001B[39m\u001B[33mrun_name\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m    402\u001B[39m             run_id=config.pop(\u001B[33m\"\u001B[39m\u001B[33mrun_id\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[32m    403\u001B[39m             **kwargs,\n\u001B[32m    404\u001B[39m         ).generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    405\u001B[39m     ).message\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\anaconda3\\envs\\llmBasic\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:381\u001B[39m, in \u001B[36mBaseChatModel._convert_input\u001B[39m\u001B[34m(self, model_input)\u001B[39m\n\u001B[32m    376\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatPromptValue(messages=convert_to_messages(model_input))\n\u001B[32m    377\u001B[39m msg = (\n\u001B[32m    378\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mInvalid input type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(model_input)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    379\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mMust be a PromptValue, str, or list of BaseMessages.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    380\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m381\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n",
      "\u001B[31mValueError\u001B[39m: Invalid input type <class 'int'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "prompt의 타입 ValueError: Invalid input type <class 'int'>. Must be a PromptValue, str, or list of BaseMessages.\n",
    "\n",
    "Prompt Template\n",
    "- PromptValue를 만들 수 있음"
   ],
   "id": "c61f67ad24111db6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:36:33.205716Z",
     "start_time": "2025-07-31T07:36:33.186222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"what is the capital of {country}\",\n",
    "    input_variable=[\"country\"],\n",
    ")"
   ],
   "id": "11a1e1558eb251",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "{country} => placeholder => template literal의 ${}와 같은 역할 => 값 대입을 어떻게 하냐? => prompt_template에 invoke를 하여 사용",
   "id": "d534bb09069a609f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:36:35.212153Z",
     "start_time": "2025-07-31T07:36:35.206156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = prompt_template.invoke({\"country\": \"France\"}) \n",
    "\n",
    "print(prompt)"
   ],
   "id": "6019593545018f1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='what is the capital of France'\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "prompt: PromptValue => prompt_template의 type",
   "id": "b56151f100ff75f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:38:23.208060Z",
     "start_time": "2025-07-31T07:38:15.012076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm.invoke(prompt_template.invoke({\n",
    "    \"country\": \"Korea\",\n",
    "}))"
   ],
   "id": "5a5b0bb871c7039d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A question that can be answered in two different ways, depending on which country you are referring to!\\n\\nThere are several countries with \"Korea\" in their name:\\n\\n1. **South Korea**: The capital of South Korea is Seoul (서울).\\n2. **North Korea**: The capital of North Korea is Pyongyang (평양).\\n\\nIf you\\'re unsure which one you were thinking of, please let me know and I\\'ll be happy to help clarify!', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-07-31T07:38:23.2038609Z', 'done': True, 'done_reason': 'stop', 'total_duration': 8184507300, 'load_duration': 2669160600, 'prompt_eval_count': 16, 'prompt_eval_duration': 277799700, 'eval_count': 91, 'eval_duration': 5236884000, 'model_name': 'llama3.1'}, id='run--0ddb6dff-21f0-4e38-86c1-4466a1357a23-0', usage_metadata={'input_tokens': 16, 'output_tokens': 91, 'total_tokens': 107})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prompt_template을 invoke한 결과를 llm에 invoke한 결과가 AI의 답변이다.",
   "id": "9f1f96306d9420e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T07:58:34.105666Z",
     "start_time": "2025-07-31T07:58:30.772102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "message_list = [\n",
    "    SystemMessage(content = \"You are a helpful assistant!\"),\n",
    "    HumanMessage(content=\"What is the capital of Germany\"),\n",
    "    AIMessage(content=\"The capital of Germany\"),\n",
    "    HumanMessage(content=\"The capital of {country}\"),\n",
    "]\n",
    "\n",
    "llm.invoke(message_list)"
   ],
   "id": "b2e5463a7d8f4bd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Berlin is the capital of Germany.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-07-31T07:58:34.1022241Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3322476700, 'load_duration': 2663699300, 'prompt_eval_count': 47, 'prompt_eval_duration': 257717400, 'eval_count': 8, 'eval_duration': 399844600, 'model_name': 'llama3.1'}, id='run--d48eaed7-cf22-4c62-a023-8e12e98fbb03-0', usage_metadata={'input_tokens': 47, 'output_tokens': 8, 'total_tokens': 55})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "HumanMessage는 BaseMessage를 상속 받음\n",
    "\n",
    "BaseMessage를 상속 받는 4가지\n",
    "\n",
    "1. system => LLM APP의 목적 => 페르소나 => LLM이 APP에서 할 목적을 정의함\n",
    "2. Human => user => 질문의 예제\n",
    "3. AI => LLM => 답변의 예제\n",
    "4. Tool => agent \n",
    "\n",
    "few_shots\n",
    "one_shot\n",
    "- shot => 예제 => LLM을 속여서 USER가 원하는 답변을 도출한다."
   ],
   "id": "89933cc6db61976f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T08:07:55.433065Z",
     "start_time": "2025-07-31T08:07:55.424561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# langChain의 ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant!\"),\n",
    "    (\"human\", \"The capital of {country}\"),\n",
    "])\n",
    "\n",
    "chat_prompt = chat_prompt_template.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(chat_prompt)"
   ],
   "id": "ebce137f77584f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='The capital of France', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "langchain의 chatprompt template을 사용할 경우 tuple을 이용해야 한다.\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant!\"),\n",
    "    (\"human\", \"The capital of {country}\"),\n",
    "])\n"
   ],
   "id": "d14d017ddda5e49e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T08:08:52.130087Z",
     "start_time": "2025-07-31T08:08:52.123739Z"
    }
   },
   "cell_type": "code",
   "source": "chat_prompt.messages",
   "id": "292ae292a7fefb26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='The capital of France', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-31T08:09:04.228402Z",
     "start_time": "2025-07-31T08:09:00.343016Z"
    }
   },
   "cell_type": "code",
   "source": "llm.invoke(chat_prompt)",
   "id": "ed82c53177648ff9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"That's an easy one!\\n\\nThe capital of France is Paris!\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-07-31T08:09:04.224838Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3875869000, 'load_duration': 2692024400, 'prompt_eval_count': 25, 'prompt_eval_duration': 390319400, 'eval_count': 14, 'eval_duration': 793003100, 'model_name': 'llama3.1'}, id='run--ae8b6347-8469-48b6-811e-7869f533b406-0', usage_metadata={'input_tokens': 25, 'output_tokens': 14, 'total_tokens': 39})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c611b575c5e0b0c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
